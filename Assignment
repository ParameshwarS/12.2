1. Difference between HBASE and HDFS.

HDFS is an integral part of Hadoop.
HDFS stands for Hadoop Distributed File System. 
HDFS is meant for storing massive amounts of data across a distributed system. 

HBase is a non-relational database that can run on top of Hadoop.
It provides you random data access/querying capabilities.
HBASE is a distributed column oriented database
running on top of HDFS.
It provides tabular form of storage of realtime semi and unstructured bigdata.
HBase stores data as key/value pairs as in a column database (something similar to Cassandra DB) while data, in HDFS is stored as flat files.

2. List and explain components of HBASE.

HBase architecture has 3 important components

HMaster
HBase HMaster assigns regions to region servers in the Hadoop cluster for load balancing. 

Responsibilities of HMaster –
It Manages and Monitors the Hadoop Cluster.(Name and Data nodes)
It acts as an Interface for creating, updating and deleting tables.
Data Definition Language operations are handled by the HMaster.
Using HMaster,client can change the schema.

Region Server
These are the worker nodes which handle read, write, update, and delete requests from clients. Region Server process, runs on every node in the hadoop cluster. Region Server runs on HDFS DataNode and consists of the following components –
Block Cache – This is the read cache. Most frequently read data is stored in the read cache and whenever the block cache is full, recently used data is evicted.
MemStore- This is the write cache and stores new data that is not yet written to the disk. Every column family in a region has a MemStore.
Write Ahead Log (WAL) is a file that stores new data that is not persisted to permanent storage.
HFile is the actual storage file that stores the rows as sorted key values on a disk.

Zookeeper
HBase uses ZooKeeper as a distributed coordination service for region assignments and to recover any region server crashes by loading them onto other region servers that are functioning. ZooKeeper is a centralized monitoring server that maintains configuration information and provides distributed synchronization. 
ZooKeeper service keeps track of all the region servers that are there in an HBase cluster- tracking information about how many region servers are there and which region servers are holding which DataNode. HMaster contacts ZooKeeper to get the details of region servers. 

3. When should we use HBASE, list some of the scenarios for the same.

It can be used When we need to perform ACID operations.
It does not require a fixed schema, without having to conform to a predefined model.
It provides users with database like access to Hadoop-scale storage.
HBase is the best choice as a NoSQL database.

4. What are the different modes in which Hbase can be run?

HBase has two run modes: 
Standalone 
Distributed
By default HBASE is in standalone mode to run in distributed mode we need to edit the configuration in conf directory.

Standalone HBase
This is the default mode.
In standalone mode, HBase does not use HDFS, it uses the local filesystem instead and it runs all HBase daemons and a local ZooKeeper all up in the same JVM. 
Zookeeper binds to a well known port so clients may talk to HBase.

Distributed
Distributed mode can be subdivided as pseudo-distributed and fully-distributed.
A pseudo-distributed mode is simply a distributed mode run on a single host. This configuration is used for testing and prototyping on HBase.
A fully-distributed mode can be run on more than one host, using the following configurations. In hbase-site.xml, add the property hbase.cluster.distributed and set it to true and point the HBase hbase.rootdir at the appropriate HDFS NameNode and location in HDFS where we would like HBase to write data.

5. Why is zookeeper needed in Hbase?

The purpose of Zookeeper is cluster management.
Zookeeper is a distributed storage that provides the following guarantees:
Sequential Consistency - Updates from a client will be applied in the order that they were sent.
Atomicity - Updates either succeed or fail. No partial results.
Single System Image - A client will see the same view of the service regardless of the server that it connects to.
Reliability - Once an update has been applied, it will persist from that time forward until a client overwrites the update.
Timeliness - The clients view of the system is guaranteed to be up-to-date within a certain time bound.

6. Hbase is a schema less database, what does it mean?

Schema less db means there is no fixed schema for the data.
In HBase,all data are stored as key value pairs and they donot have a fixed schema.

7. What is the minimum number of column family every Hbase table should have?

Atleast one column family is needed.

8. What is the benefit of using connection pool in Hbase?

For applications which require high-end multithreaded access 
We can pre-create connections for those applications.

9. What is the difference between memstore and hfile in HBase?

HFile
File format for hbase. 
A file of sorted key/value pairs. 
Both keys and values are byte arrays.
When the MemStore accumulates enough data, the entire sorted KeyValue set is written 
to a new HFile in HDFS.
This is a sequential write.It is very fast, as it avoids moving the disk drive head.

Memstore
When something is written to HBase,it is first written to an in-memory store (memstore).
After once this memstore reaches a certain size,it is flushed to disk into a store file
The store files created on disk are immutable. 

10.Describe compactions in HBase.

When something is written to HBase, 
it is first written to an in-memory store (memstore).
After this memstore reaches a certain size, it is flushed to disk into a store file
The store files created on disk are immutable.
Sometimes the store files are merged together,this is done by a process called compaction.

There are two kinds of compactions:
Minor compactions: these are triggered each time a memstore is flushed, and will merge some of the store files, determined by an algorithm described below.
Major compactions: these run about every 24 hours (after the currently oldest store file was written), and merge together all store files into one. The 24 hours is adjusted with a random margin of up to 20% to avoid many major compactions happening at the same time. Major compactions can also be triggered manually, via the API or the shell.

11.List and explain the logical entities in HBase.

Tables – The HBase Tables are more like logical collection of rows stored in separate partitions called Regions.
Rows
A row is one instance of data in a table and is identified by a rowkey.
Column Families
Data in a row are grouped together as Column Families.
Columns
A Column Family is made of one or more columns. A Column is identified by a Column Qualifier that consists of the Column Family name concatenated with the Column name using a colon
Cell
A Cell stores data and is essentially a unique combination of rowkey, Column Family and the Column
Version
The data stored in a cell is versioned and versions of data are identified by the timestamp.
RowKey
Rows are identified uniquely by their row key
Column Family
Data within a row is grouped by column family

12.What will happen if we do not create a row key while inserting the data?

HBase stores data as key/value pairs as in a column database.
Here,row key acts as key and column values acts as value.
So its not possible to insert the data without a row key.

13.How can filters be applied in HBase and what are the benefits?

It allows to perform server-side filtering when accessing HBase over Thrift or in the HBase shell.

14.What are the data model operations in hBase?

Get :
Get returns attributes for a specified row.
Put:
Put either adds new rows to a table or can update existing rows.
Scan:
Scan allow iteration over multiple rows for specified attributes. 
Delete:
Delete removes a row from a table.
Deletes are executed via HTable.delete. 

15.How MapReduce can be used with HBase?

Answer:
Mapreduce jobs that uses HBase can be run by adding the
HBase JAR files
and Zookeeper JAR files to the Hadoop Java classpath.

16.What is regionserver? 

Answer:
They are the worker nodes.
They handle read, write, update, and delete requests from clients.
Region Server process, runs on every node in the hadoop cluster.
Regions are assigned to the Regoin Servers.
A region server can serve about 1,000 regions.
